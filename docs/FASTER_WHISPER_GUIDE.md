# ğŸš€ ×”××“×¨×™×š ×”××œ× ×œ×¢×‘×•×“×” ×¢× faster-whisper

## ğŸ“‹ **××” ×–×” faster-whisper?**

faster-whisper ×”×•× ×™×™×©×•× ××—×“×© ×©×œ ××•×“×œ Whisper ×©×œ OpenAI ×‘×××¦×¢×•×ª CTranslate2, ×©×”×•× ×× ×•×¢ inference ××”×™×¨ ×œ××•×“×œ×™× ×©×œ Transformer. ×”×™×™×©×•× ×”×–×” ××”×™×¨ ×¢×“ ×¤×™ 4 ×-openai/whisper ×¢× ××•×ª×” ×“×™×•×§ ×•××©×ª××© ×‘×¤×—×•×ª ×–×™×›×¨×•×Ÿ.

---

## ğŸ› ï¸ **×”×ª×§× ×” ××”×™×¨×”**

### **âš ï¸ ×”×¢×¨×•×ª ×—×©×•×‘×•×ª ×œ×’×‘×™ Python:**
- **Python 3.13:** ×œ× × ×ª××š ×›×¨×’×¢ (ctranslate2 ×œ× ×ª×•××š)
- **××•××œ×¥:** Python 3.10, 3.11 ××• 3.12

### **×©×œ×‘ 1: ×”×ª×§× ×” ×‘×¡×™×¡×™×ª**
```bash
# ×”×ª×§× ×” ×¡×˜× ×“×¨×˜×™×ª
pip install faster-whisper

# ××• ×”×ª×§× ×” ××”×§×•×“ ×”×—×“×© ×‘×™×•×ª×¨
pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz"
```

### **×©×œ×‘ 2: ×ª×œ×•×™×•×ª × ×•×¡×¤×•×ª (××•×¤×¦×™×•× ×œ×™)**
```bash
# ×¢×‘×•×¨ GPU (NVIDIA) - ×‘×“×•×§ ×ª××™××•×ª CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ×¢×‘×•×¨ ×§×‘×¦×™ ×©××¢ × ×•×¡×¤×™×
pip install librosa soundfile
```

---

## âš¡ **×©×™××•×© ×‘×¡×™×¡×™ ××•×˜×‘**

### **×“×•×’××” ×¤×©×•×˜×” - ××™×˜×‘×™×ª ×œCPU:**
```python
from faster_whisper import WhisperModel

# ×”×’×“×¨×•×ª ××™×˜×‘×™×•×ª ×œCPU
model_size = "base"  # ××• "small", "medium" ×œ×¤×™ ×”×¦×•×¨×š
model = WhisperModel(
    model_size, 
    device="cpu", 
    compute_type="int8",    # ğŸ”¥ ×¤×™ 2 ××”×™×¨ ×™×•×ª×¨!
    num_workers=1,          # ××™×˜×‘×™ ×œ××©×™××•×ª ×›×‘×“×•×ª
    download_root="./models"  # cache ××§×•××™
)

# ×ª××œ×•×œ ××™×˜×‘×™
segments, info = model.transcribe(
    "audio.mp3", 
    beam_size=5,                    # ×‘×¨×™×¨×ª ××—×“×œ ×‘-faster-whisper (×œ× 1!)
    language="auto",                # ×–×™×”×•×™ ××•×˜×•××˜×™
    condition_on_previous_text=False,  # ××”×™×¨ ×™×•×ª×¨
    vad_filter=True,                # ×¡×™× ×•×Ÿ ×¨×¢×© ×˜×•×‘ ×™×•×ª×¨
    vad_parameters=dict(min_silence_duration_ms=500)
)

print(f"×©×¤×” ××–×•×”×”: {info.language} ({info.language_probability:.2f})")

# ×”×“×¤×¡×ª ×ª×•×¦××•×ª
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
```

### **×“×•×’××” ××ª×§×“××ª ×¢× ××•×¤×˜×™××™×–×¦×™×•×ª:**
```python
from faster_whisper import WhisperModel
import time

class OptimizedWhisper:
    def __init__(self, model_size="base", device="cpu"):
        self.model = WhisperModel(
            model_size,
            device=device,
            compute_type="int8" if device == "cpu" else "float16",
            num_workers=1,
            local_files_only=False,
            download_root="./whisper_models"
        )
    
    def transcribe_optimized(self, audio_path, target_language="he"):
        start_time = time.time()
        
        # ×”×’×“×¨×•×ª ××™×˜×‘×™×•×ª
        segments, info = self.model.transcribe(
            audio_path,
            beam_size=5,                      # ×‘×¨×™×¨×ª ××—×“×œ ×‘-faster-whisper
            best_of=1,                        # ×œ×œ× ×—×™×¤×•×©×™× × ×•×¡×¤×™×
            temperature=0.0,                  # deterministic
            condition_on_previous_text=False, # ××”×™×¨ ×™×•×ª×¨
            compression_ratio_threshold=2.4,  # ×–×™×”×•×™ hallucinations
            log_prob_threshold=-1.0,          # ×¡×™× ×•×Ÿ ×ª×•×¦××•×ª ×—×œ×©×•×ª
            no_speech_threshold=0.6,          # ×–×™×”×•×™ ×©×§×˜
            vad_filter=True,                  # ×—×©×•×‘ ×œ××™×›×•×ª
            vad_parameters=dict(
                min_silence_duration_ms=500,
                speech_pad_ms=400
            )
        )
        
        processing_time = time.time() - start_time
        
        # ×”××¨×” ×œ×¨×©×™××” (segments ×”×•× generator)
        segments_list = list(segments)
        
        print(f"â±ï¸ ×–××Ÿ ×¢×™×‘×•×“: {processing_time:.2f} ×©× ×™×•×ª")
        print(f"ğŸ¯ ×©×¤×”: {info.language} (×‘×™×˜×—×•×Ÿ: {info.language_probability:.2f})")
        print(f"ğŸ“Š {len(segments_list)} segments")
        
        return segments_list, info
    
    def save_to_file(self, segments, output_path):
        """×©××™×¨×” ×œ×§×•×‘×¥ ×˜×§×¡×˜"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for segment in segments:
                f.write(f"{segment.text.strip()}\n")
        print(f"âœ… × ×©××¨ ×‘: {output_path}")

# ×©×™××•×©
whisper = OptimizedWhisper(model_size="base")
segments, info = whisper.transcribe_optimized("audio.mp3")
whisper.save_to_file(segments, "transcript.txt")
```

---

## ğŸ¯ **×”×’×“×¨×•×ª ××™×˜×‘×™×•×ª ×œ×¤×™ ××§×¨×” ×©×™××•×©**

### **ğŸƒâ€â™‚ï¸ ××”×™×¨×•×ª ××§×¡×™××œ×™×ª (real-time):**
```python
model = WhisperModel(
    "tiny",                    # ××•×“×œ ×”×›×™ ×§×˜×Ÿ
    device="cpu",
    compute_type="int8",       # ×“×—×™×¡×” ××§×¡×™××œ×™×ª
    num_workers=1
)

segments, info = model.transcribe(
    audio_path,
    beam_size=1,              # ×”×›×™ ××”×™×¨ (×œ× ×‘×¨×™×¨×ª ×”××—×“×œ!)
    best_of=1,
    temperature=0.0,
    language="he",            # ×”×’×“×¨×” ×™×“× ×™×ª (××”×™×¨ ×™×•×ª×¨)
    condition_on_previous_text=False,
    vad_filter=False          # ××”×™×¨ ×™×•×ª×¨ ××‘×œ ×¤×—×•×ª ××™×›×•×ª
)
```

### **ğŸ¯ ××™×›×•×ª ××§×¡×™××œ×™×ª:**
```python
model = WhisperModel(
    "large-v3",               # ××•×“×œ ×”×›×™ ×˜×•×‘
    device="cpu",
    compute_type="int8",      # ×¢×“×™×™×Ÿ ××”×™×¨
    num_workers=1
)

segments, info = model.transcribe(
    audio_path,
    beam_size=5,              # ×‘×¨×™×¨×ª ××—×“×œ ×‘-faster-whisper
    best_of=5,                # ××¡×¤×¨ × ×™×¡×™×•× ×•×ª
    temperature=[0.0, 0.2, 0.4, 0.6, 0.8],  # ××¡×¤×¨ ×˜××¤×¨×˜×•×¨×•×ª
    condition_on_previous_text=True,         # ×”×§×©×¨
    vad_filter=True,          # ×¡×™× ×•×Ÿ ×¨×¢×©
    vad_parameters=dict(
        min_silence_duration_ms=300,
        speech_pad_ms=400
    )
)
```

### **âš–ï¸ ××™×–×•×Ÿ ××•×©×œ× (××•××œ×¥):**
```python
model = WhisperModel(
    "base",                   # ××™×–×•×Ÿ ×˜×•×‘
    device="cpu",
    compute_type="int8",
    num_workers=1
)

segments, info = model.transcribe(
    audio_path,
    beam_size=2,              # ××™×–×•×Ÿ ××”×™×¨×•×ª/××™×›×•×ª
    temperature=0.0,
    language="auto",          # ×–×™×”×•×™ ××•×˜×•××˜×™
    condition_on_previous_text=False,
    vad_filter=True,
    vad_parameters=dict(min_silence_duration_ms=500)
)
```

---

## ğŸš€ **××•×¤×˜×™××™×–×¦×™×•×ª ××ª×§×“××•×ª**

### **1. ğŸ“¦ Batching ×¢×‘×•×¨ ×§×‘×¦×™× ××¨×•×‘×™× (×’×¨×¡××•×ª ×—×“×©×•×ª ×‘×œ×‘×“):**
```python
# âš ï¸ ×–××™×Ÿ ×¨×§ ×‘×’×¨×¡××•×ª ×××•×“ ×—×“×©×•×ª ×©×œ faster-whisper
try:
    from faster_whisper import BatchedInferencePipeline
    
    # ×™×¦×™×¨×ª pipeline ××§×‘×™×œ×™
    batched_model = BatchedInferencePipeline(
        model=model,
        chunk_length=30,          # ××•×¨×š ×—×œ×§×™×
        stride_length=5,          # ×—×¤×™×¤×”
        batch_size=16             # ×›××•×ª ××§×‘×™×œ×™×ª
    )
    
    # ×¢×™×‘×•×“ ××§×‘×™×œ×™
    segments, info = batched_model.transcribe("long_audio.mp3")
except ImportError:
    print("BatchedInferencePipeline ×œ× ×–××™×Ÿ ×‘×’×¨×¡×” ×–×•")
```

### **2. ğŸ›ï¸ × ×™×”×•×œ ×–×™×›×¨×•×Ÿ ×—×›×:**
```python
import gc
import torch

class MemoryEfficientWhisper:
    def __init__(self, model_size="base"):
        self.model_size = model_size
        self.model = None
    
    def load_model(self):
        if self.model is None:
            self.model = WhisperModel(
                self.model_size,
                device="cpu",
                compute_type="int8",
                num_workers=1
            )
    
    def unload_model(self):
        if self.model is not None:
            del self.model
            self.model = None
            gc.collect()  # × ×™×§×•×™ ×–×™×›×¨×•×Ÿ
    
    def transcribe_with_cleanup(self, audio_path):
        try:
            self.load_model()
            segments, info = self.model.transcribe(audio_path)
            return list(segments), info
        finally:
            self.unload_model()  # × ×™×§×•×™ ××•×˜×•××˜×™
```

### **3. ğŸ“ˆ × ×™×˜×•×¨ ×‘×™×¦×•×¢×™×:**
```python
import psutil
import time

def monitor_performance(func):
    def wrapper(*args, **kwargs):
        # ××“×™×“×ª ×–×™×›×¨×•×Ÿ ×œ×¤× ×™
        memory_before = psutil.virtual_memory().used / 1024 / 1024  # MB
        cpu_before = psutil.cpu_percent()
        start_time = time.time()
        
        # ×”×¨×¦×”
        result = func(*args, **kwargs)
        
        # ××“×™×“×ª ×‘×™×¦×•×¢×™× ××—×¨×™
        end_time = time.time()
        memory_after = psutil.virtual_memory().used / 1024 / 1024
        cpu_after = psutil.cpu_percent()
        
        print(f"â±ï¸ ×–××Ÿ: {end_time - start_time:.2f}s")
        print(f"ğŸ’¾ ×–×™×›×¨×•×Ÿ: {memory_after - memory_before:.1f}MB")
        print(f"ğŸ”¥ CPU: {(cpu_before + cpu_after) / 2:.1f}%")
        
        return result
    return wrapper

@monitor_performance
def transcribe_with_monitoring(audio_path):
    model = WhisperModel("base", device="cpu", compute_type="int8")
    segments, info = model.transcribe(audio_path)
    return list(segments), info
```

---

## ğŸ”§ **×˜×™×¤×™× ××¢×©×™×™× ×œ×¤×¨×•×“×§×©×Ÿ**

### **1. âš¡ cache ××•×“×œ×™×:**
```python
# Singleton pattern ×œ××•×“×œ
class WhisperSingleton:
    _instance = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_model(self, model_size="base"):
        if self._model is None:
            self._model = WhisperModel(
                model_size,
                device="cpu", 
                compute_type="int8"
            )
        return self._model

# ×©×™××•×©
whisper = WhisperSingleton()
model = whisper.get_model()
```

### **2. ğŸ›¡ï¸ ×˜×™×¤×•×œ ×‘×©×’×™××•×ª:**
```python
def safe_transcribe(audio_path, max_retries=3):
    for attempt in range(max_retries):
        try:
            model = WhisperModel("base", device="cpu", compute_type="int8")
            segments, info = model.transcribe(audio_path)
            return list(segments), info
        except Exception as e:
            print(f"× ×™×¡×™×•×Ÿ {attempt + 1} × ×›×©×œ: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(1)  # ×”××ª× ×” ×œ×¤× ×™ × ×™×¡×™×•×Ÿ ×—×•×–×¨
```

### **3. ğŸšï¸ ×”×’×“×¨×•×ª ××•×ª×××•×ª ×œ×©×¤×”:**
```python
LANGUAGE_CONFIGS = {
    "he": {  # ×¢×‘×¨×™×ª
        "model_size": "large-v3",    # ×“×™×•×§ ×˜×•×‘ ×™×•×ª×¨ ×œ×¢×‘×¨×™×ª
        "beam_size": 3,
        "condition_on_previous_text": True,  # ×—×©×•×‘ ×œ×¢×‘×¨×™×ª
        "vad_filter": True
    },
    "en": {  # ×× ×’×œ×™×ª
        "model_size": "base",
        "beam_size": 2,              # ××™×–×•×Ÿ ×˜×•×‘
        "condition_on_previous_text": False,
        "vad_filter": True
    },
    "ar": {  # ×¢×¨×‘×™×ª
        "model_size": "large-v3",
        "beam_size": 5,
        "condition_on_previous_text": True,
        "vad_filter": True
    }
}

def transcribe_by_language(audio_path, language="auto"):
    config = LANGUAGE_CONFIGS.get(language, LANGUAGE_CONFIGS["en"])
    
    model = WhisperModel(
        config["model_size"],
        device="cpu",
        compute_type="int8"
    )
    
    segments, info = model.transcribe(
        audio_path,
        beam_size=config["beam_size"],
        language=language if language != "auto" else None,
        condition_on_previous_text=config["condition_on_previous_text"],
        vad_filter=config["vad_filter"]
    )
    
    return list(segments), info
```

---

## ğŸ“Š **×”×©×•×•××ª ×‘×™×¦×•×¢×™× ××¦×•×¤×™×**

### **××”×™×¨×•×ª ×œ×¤×™ ××•×“×œ (×•×™×“××• 10 ×“×§×•×ª):**

| **××•×“×œ** | **×–××Ÿ ×¢×™×‘×•×“** | **××™×›×•×ª** | **×–×™×›×¨×•×Ÿ** | **××•××œ×¥ ×œ** |
|-----------|----------------|-----------|-------------|-------------|
| `tiny` | 2-3 ×“×§×•×ª | ×‘×¡×™×¡×™ | 200MB | real-time |
| `base` | 4-5 ×“×§×•×ª | ×˜×•×‘ | 400MB | **×¢×‘×•×“×” ×™×•××™×•××™×ª** |
| `small` | 7-8 ×“×§×•×ª | ×˜×•×‘ ×××•×“ | 600MB | ××™×›×•×ª ×’×‘×•×”×” |
| `medium` | 12-15 ×“×§×•×ª | ××¢×•×œ×” | 1GB | ×¢×™×‘×•×“ ××§×¦×•×¢×™ |
| `large-v3` | 20-25 ×“×§×•×ª | ×”×˜×•×‘ ×‘×™×•×ª×¨ | 2GB | ×“×™×•×§ ××§×¡×™××œ×™ |

### **××•×¤×˜×™××™×–×¦×™×•×ª ××¦×˜×‘×¨×•×ª:**
- **`compute_type="int8"`**: +50% ××”×™×¨×•×ª âœ…
- **`beam_size=1`**: +60% ××”×™×¨×•×ª (×‘××§×•× ×‘×¨×™×¨×ª ××—×“×œ 5) âœ…
- **`vad_filter=True`**: +20% ××™×›×•×ª âœ…
- **`condition_on_previous_text=False`**: +15% ××”×™×¨×•×ª âœ…

---

## ğŸ¯ **×××¦××™× ×—×©×•×‘×™× - ×¢×“×›×•×Ÿ ××“×•×™×§:**

### **âš ï¸ ×”×‘×“×œ×™× ×—×©×•×‘×™× ×-OpenAI Whisper:**
1. **beam_size:** ×‘×¨×™×¨×ª ××—×“×œ ×‘-faster-whisper ×”×•× **5** (×œ× 1 ×›××• ×‘-OpenAI)
2. **×”×©×•×•××” ×”×•×’× ×ª:** ×œ×”×©×•×•××ª ××”×™×¨×•×ª, ×”×©×ª××© ×‘-`beam_size=1`
3. **BatchedInferencePipeline:** ×–××™×Ÿ ×¨×§ ×‘×’×¨×¡××•×ª ×××•×“ ×—×“×©×•×ª
4. **Python 3.13:** ×œ× × ×ª××š ×›×¨×’×¢

### **ğŸ“Š ×”×©×•×•××ª beam_size:**
```python
# ××”×™×¨ ×‘×™×•×ª×¨ (×”×©×•×•××” ×”×•×’× ×ª ×œ-OpenAI Whisper)
segments, info = model.transcribe("audio.mp3", beam_size=1)

# ××™×›×•×ª ×˜×•×‘×” ×™×•×ª×¨ (×‘×¨×™×¨×ª ××—×“×œ ×‘-faster-whisper)
segments, info = model.transcribe("audio.mp3", beam_size=5)

# ××™×–×•×Ÿ ××•×©×œ×
segments, info = model.transcribe("audio.mp3", beam_size=2)
```

---

## ğŸ¯ **×”××œ×¦×•×ª ×œ×¤×¨×•×™×§×˜ ×©×œ×›×**

### **×œ×”×—×œ×¤×” ×‘××¢×¨×›×ª ×”× ×•×›×—×ª:**
```python
# ×‘××§×•×:
import whisper
model = whisper.load_model("base")
result = model.transcribe(audio_path)

# ×”×©×ª××© ×‘:
from faster_whisper import WhisperModel
model = WhisperModel("base", device="cpu", compute_type="int8")
segments, info = model.transcribe(
    audio_path, 
    beam_size=1,                      # ×œ×”×©×•×•××” ×”×•×’× ×ª ×¢× OpenAI
    condition_on_previous_text=False,
    vad_filter=True
)

# ×”××¨×” ×œ×¤×•×¨××˜ ×“×•××”
result = {
    'text': ' '.join([seg.text for seg in segments]),
    'segments': [
        {
            'start': seg.start,
            'end': seg.end, 
            'text': seg.text
        } for seg in segments
    ],
    'language': info.language
}
```

**×”×ª×•×¦××”:** **×¤×™ 2-4 ××”×™×¨ ×™×•×ª×¨** ×¢× **××•×ª×” ××™×›×•×ª ×‘×“×™×•×§**! ğŸš€

---

## âœ… **×¡×™×›×•× ×”×¢×¨×›×”:**

### **××” ×©××“×•×™×§ ×‘××“×¨×™×š:**
1. âœ… **compute_type="int8"** ××”×™×¨ ×¤×™ 2 ×‘-CPU
2. âœ… **vad_filter=True** ×©×™×¤×•×¨ ××™×›×•×ª ××©××¢×•×ª×™
3. âœ… **××”×™×¨×•×ª ×¤×™ 4** ×-OpenAI Whisper (×¢× ×”×’×“×¨×•×ª × ×›×•× ×•×ª)
4. âœ… **×ª××™×›×” ×‘-quantization**

### **×ª×™×§×•× ×™× ×—×©×•×‘×™×:**
1. âš ï¸ **beam_size:** ×‘×¨×™×¨×ª ××—×“×œ ×”×™× 5 (×œ× 1)
2. âš ï¸ **Python 3.13:** ×œ× × ×ª××š ×›×¨×’×¢
3. âš ï¸ **BatchedInferencePipeline:** ×–××™×Ÿ ×¨×§ ×‘×’×¨×¡××•×ª ×—×“×©×•×ª
4. âš ï¸ **beam_size=1:** ×œ×”×©×•×•××” ×”×•×’× ×ª ×¢× OpenAI Whisper

### **×”××œ×¦×” ×¡×•×¤×™×ª:**
×œ×”×©×’×ª ×”××”×™×¨×•×ª ×”××§×¡×™××œ×™×ª ×ª×•×š ×©××™×¨×” ×¢×œ ××™×›×•×ª ×˜×•×‘×”:
- **××•×“×œ:** `base` ××• `small`
- **beam_size:** `1` ×œ××”×™×¨×•×ª, `2` ×œ××™×–×•×Ÿ, `5` ×œ××™×›×•×ª
- **compute_type:** `int8`
- **vad_filter:** `True`
